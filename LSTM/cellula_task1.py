# -*- coding: utf-8 -*-
"""Cellula_task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TUj6NCbRykPulKIo8bGZM0Q76pXNNtPk
"""

import pandas as pd
import numpy as np

df = pd.read_csv("/content/cellula toxic data (1).csv")
df.head()

df.info()
df.isnull().sum()

df.duplicated().sum() #checking for duplicates

df[df.duplicated()]

df[df.duplicated(keep=False)].sort_values(by=df.columns.tolist())

df.drop_duplicates().shape

"""After removing duplicates we know have 2027 unique rows and 3 columns

"""

df["query"].value_counts().head(10)

df["Toxic Category"].value_counts()

#converting each Toxic Category into a number
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df["label"] = le.fit_transform(df["Toxic Category"])
label_map = {label: idx for idx, label in enumerate(le.classes_)}
label_map

#Splitting the dataset into 80% training data and 20% test data
from sklearn.model_selection import train_test_split
df["text"] = df["query"] + " " + df["image descriptions"]
x_train, x_test, y_train, y_test = train_test_split(
    df["text"] ,
    df["label"],
    test_size=0.2,
    random_state=42,
    stratify=df["label"] #splitting the data while preserving the class distribution instead of splitting randomly (for F1 score)
)

#Tokenization with converting text to tensors
from tensorflow.keras.preprocessing.text import Tokenizer

MAX_WORDS = 20000
MAX_LEN = 100

tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token="<OOV>")
tokenizer.fit_on_texts(x_train)

x_train_seq = tokenizer.texts_to_sequences(x_train)
x_test_seq  = tokenizer.texts_to_sequences(x_test)

#Padding because sentences have different lenghts and LSTM requires same-length inputs
#Adds fake tokens (zeros) to shorter sequences so now every sequence will be 100 tokens long
from tensorflow.keras.preprocessing.sequence import pad_sequences

x_train_pad = pad_sequences(x_train_seq, maxlen=MAX_LEN, padding="post") #after sentence
x_test_pad  = pad_sequences(x_test_seq,  maxlen=MAX_LEN, padding="post")

# To convert word IDs into dense vectors that capture meaning
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.models import Sequential

num_classes = df["label"].nunique() #number of labels to match with the output layer
model = Sequential(
    [ Embedding(input_dim=MAX_WORDS, output_dim=128, input_length=100),
     Bidirectional(LSTM(64)),
     Dropout(0.2), #turns off 20% of neurons during training to prevent overfitting
     Dense(num_classes, activation="softmax") ]) #converts LSTM output to class probabilities
model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

#Training the model and saving results in a record of training statistics
history = model.fit(
    x_train_pad,
    y_train,
    epochs=15,
    batch_size=32,
    validation_split=0.1
)

# Prediction
from sklearn.metrics import f1_score, confusion_matrix
y_pred_prob = model.predict(x_test_pad)
y_pred = np.argmax(y_pred_prob, axis=1)
f1 = f1_score(y_test, y_pred, average="macro")
print("Macro F1 Score:", f1)

# Training curves
import matplotlib.pyplot as plt

plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.legend()
plt.title("Loss Curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()

import seaborn as sns
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8,6))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=le.classes_,
    yticklabels=le.classes_
)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()